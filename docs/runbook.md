# Runbook Operacional - TRF5 Scraper

Este documento fornece instruï¿½ï¿½es operacionais detalhadas para execuï¿½ï¿½o, monitoramento e troubleshooting do TRF5 Scraper.

## Checklist de Pre-Execucao

Antes de executar qualquer comando, verifique:

```bash
# 1. Ambiente virtual ativo
source .venv/bin/activate

# 2. MongoDB rodando
docker ps | grep mongo
# ou
mongosh --eval "db.runCommand('ping')"

# 3. Conectividade com TRF5
curl -I http://www5.trf5.jus.br/cp/

# 4. Spiders disponï¿½veis
scrapy list
# Deve retornar: trf5, parse_raw

# 5. Variï¿½veis de ambiente
echo $MONGO_URI
echo $MONGO_DB
```

## Comandos de Execucao

### 1. Busca por NPU (Teste Individual)

```bash
# Comando base
scrapy crawl trf5 \
  -a modo=numero \
  -a valor="0015648-78.1999.4.05.0000" \
  -s LOG_LEVEL=INFO

# Com log em arquivo
scrapy crawl trf5 \
  -a modo=numero \
  -a valor="0015648-78.1999.4.05.0000" \
  -s LOG_LEVEL=INFO \
  -s LOG_FILE=logs/npu_$(date +%Y%m%d_%H%M%S).log

# Teste de idempotï¿½ncia (executar 2x o mesmo)
scrapy crawl trf5 -a modo=numero -a valor="0015648-78.1999.4.05.0000" -s LOG_LEVEL=INFO
# Primeira execuï¿½ï¿½o: deve logar "insert"
# Segunda execuï¿½ï¿½o: deve logar "update"
```

### 2. Descoberta por CNPJ

```bash
# Comando base com limites
scrapy crawl trf5 \
  -a modo=cnpj \
  -a valor="00.000.000/0001-91" \
  -a max_pages=2 \
  -a max_details_per_page=5 \
  -s LOG_LEVEL=INFO

# Teste ampliado (cuidado com rate limiting)
scrapy crawl trf5 \
  -a modo=cnpj \
  -a valor="00.000.000/0001-91" \
  -a max_pages=5 \
  -a max_details_per_page=10 \
  -s LOG_LEVEL=INFO \
  -s LOG_FILE=logs/cnpj_$(date +%Y%m%d_%H%M%S).log
```

### 3. Reprocessamento Offline

```bash
# Reprocessar ï¿½ltimas 10 pï¿½ginas
scrapy crawl parse_raw -a limit=10 -s LOG_LEVEL=INFO

# Reprocessar por tipo especï¿½fico
scrapy crawl parse_raw -a limit=20 -a tipo=detalhe -s LOG_LEVEL=INFO

# Reprocessar por tipo de busca
scrapy crawl parse_raw -a limit=15 -a busca=numero -s LOG_LEVEL=INFO
```

## =' Scripts de Automaï¿½ï¿½o

### Executar Todos os NPUs do BB

```bash
./scripts/run_npu.sh
```

**O que faz:**
- Executa todos os 6 NPUs fornecidos pelo Banco do Brasil
- Testa idempotï¿½ncia executando cada NPU 2 vezes
- Salva logs individuais para cada execuï¿½ï¿½o
- Gera relatï¿½rio final de sucesso/falha

### Descoberta por CNPJ com Limites

```bash
./scripts/run_cnpj.sh
```

**O que faz:**
- Executa descoberta pelo CNPJ do Banco do Brasil
- Aplica limites seguros (max_pages=2, max_details_per_page=5)
- Monitora paginaï¿½ï¿½o e classificaï¿½ï¿½o de pï¿½ginas
- Salva evidï¿½ncias de listas e detalhes coletados

### Reprocessamento Offline

```bash
./scripts/reprocess_offline.sh
```

**O que faz:**
- Reprocessa pï¿½ginas HTML salvas sem fazer requisiï¿½ï¿½es de rede
- Testa a robustez dos extractors
- ï¿½til para debug e refinamento dos parsers

### Consultas MongoDB Rï¿½pidas

```bash
./scripts/mongo_queries.sh
```

**O que faz:**
- Executa consultas padrï¿½o de verificaï¿½ï¿½o
- Mostra ï¿½ltimas pï¿½ginas coletadas
- Lista processos extraï¿½dos
- Verifica integridade dos dados

## =ï¿½ Monitoramento Durante Execuï¿½ï¿½o

### Logs Crï¿½ticos a Observar

#### 1. Iniciaï¿½ï¿½o do Spider
```
[trf5] INFO: Iniciando coleta TRF5 (modo=numero, valor=0015648-78.1999.4.05.0000, max_pages=5, max_details=10)
```

#### 2. Classificaï¿½ï¿½o de Pï¿½ginas
```
[trf5] INFO: Pï¿½gina de lista processada (page=0, tipo=list, url=http://...)
[trf5] INFO: Pï¿½gina nï¿½o ï¿½ detalhe conforme esperado: http://...
```

#### 3. Paginaï¿½ï¿½o Detectada
```
[trf5] INFO: Paginaï¿½ï¿½o detectada: Total=157, last_page=15
[trf5] INFO: Extraï¿½dos 5 links de detalhe desta pï¿½gina (total coletado: 12)
```

#### 4. Persistï¿½ncia MongoDB
```
[mongo] INFO: [raw] saved detalhe (0015648-78.1999.4.05.0000) http://...
[mongo] INFO: [processos] insert _id=0015648-78.1999.4.05.0000 relator=Joï¿½o da Silva
[mongo] INFO: [processos] update _id=0015648-78.1999.4.05.0000 relator=Joï¿½o da Silva
```

#### 5. Rate Limiting e Cortesia
```
[scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET http://...>
[scrapy.extensions.throttle] INFO: AutoThrottle: Adjusting delay to 1.2s
```

### Sinais de Problema

#### L Conectividade
```
twisted.internet.error.ConnectError: An error occurred
scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://...>
```

#### L Classificaï¿½ï¿½o Errada
```
[trf5] WARNING: Pï¿½gina nï¿½o ï¿½ lista conforme esperado: http://...
[trf5] WARNING: Pï¿½gina de erro detectada: http://...
```

#### L Extraï¿½ï¿½o Falhou
```
[trf5] WARNING: Nï¿½mero do processo nï¿½o encontrado em http://...
[trf5] ERROR: Erro ao extrair dados do processo http://... KeyError: 'relator'
```

#### L MongoDB Indisponï¿½vel
```
[mongo] ERROR: Erro ao conectar no MongoDB: ServerSelectionTimeoutError
```

## =ï¿½ Consultas MongoDB de Verificaï¿½ï¿½o

### Conectar ao MongoDB

```bash
# Via mongosh local
mongosh "mongodb://localhost:27017/trf5"

# Via Docker
docker exec -it trf5-mongo mongosh "mongodb://trf5:trf5pass@localhost:27017/trf5?authSource=admin"
```

### Consultas Essenciais

#### 1. Verificar ï¿½ltimas Pï¿½ginas Coletadas
```javascript
db.raw_pages.find({}, {
  url: 1,
  "context.tipo": 1,
  "context.busca": 1,
  "context.numero": 1,
  "context.cnpj": 1,
  "context.page_idx": 1,
  fetched_at: 1
}).sort({_id: -1}).limit(10).toArray()
```

#### 2. Contar Pï¿½ginas por Tipo
```javascript
db.raw_pages.aggregate([
  {$group: {
    _id: "$context.tipo",
    count: {$sum: 1}
  }}
])
```

#### 3. Verificar Processos Extraï¿½dos
```javascript
db.processos.find({}, {
  numero_processo: 1,
  relator: 1,
  data_autuacao: 1,
  "envolvidos.0.papel": 1,
  "movimentacoes.0.data": 1,
  scraped_at: 1
}).sort({_id: -1}).limit(5).toArray()
```

#### 4. Validar NPUs do Banco do Brasil
```javascript
var npus_bb = [
  "0015648-78.1999.4.05.0000",
  "0012656-90.2012.4.05.0000",
  "0043753-74.2013.4.05.0000",
  "0002098-07.2011.4.05.8500",
  "0460674-33.2019.4.05.0000",
  "0000560-67.2017.4.05.0000"
];

npus_bb.forEach(npu => {
  var doc = db.processos.findOne({_id: npu});
  print(`${npu}: ${doc ? 'OK' : 'MISSING'}`);
});
```

#### 5. Verificar Qualidade dos Dados
```javascript
// Processos sem relator
db.processos.find({$or: [{relator: null}, {relator: ""}]}).count()

// Processos sem envolvidos
db.processos.find({$or: [{envolvidos: null}, {envolvidos: []}]}).count()

// Processos sem movimentaï¿½ï¿½es
db.processos.find({$or: [{movimentacoes: null}, {movimentacoes: []}]}).count()

// Datas nï¿½o ISO
db.processos.find({data_autuacao: {$not: /^\d{4}-\d{2}-\d{2}/}}).count()
```

## =ï¿½ Troubleshooting

### Problema: Scrapy nï¿½o encontra spiders

**Sintoma:**
```
AttributeError: 'NoneType' object has no attribute 'split'
spider not found: trf5
```

**Soluï¿½ï¿½o:**
```bash
# Verificar estrutura do projeto
ls -la trf5_scraper/spiders/
cat scrapy.cfg

# Recompiliar Python
python3 -m py_compile trf5_scraper/spiders/trf5.py
```

### Problema: MongoDB connection failed

**Sintoma:**
```
pymongo.errors.ServerSelectionTimeoutError: localhost:27017
```

**Soluï¿½ï¿½o:**
```bash
# Verificar status
docker ps | grep mongo

# Restart se necessï¿½rio
docker compose -f docker/compose.yaml restart

# Verificar logs
docker logs trf5-mongo
```

### Problema: TRF5 site inacessï¿½vel

**Sintoma:**
```
twisted.internet.error.DNSLookupError
```

**Soluï¿½ï¿½o:**
```bash
# Testar conectividade
ping www5.trf5.jus.br
curl -v http://www5.trf5.jus.br/cp/

# Verificar proxy/firewall
echo $http_proxy
echo $https_proxy
```

### Problema: Rate limiting / 429 errors

**Sintoma:**
```
[scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://...> (failed 1 times): 429
```

**Soluï¿½ï¿½o:**
```bash
# Aumentar delays
scrapy crawl trf5 -a modo=... -s DOWNLOAD_DELAY=2.0 -s AUTOTHROTTLE_START_DELAY=3.0

# Reduzir concorrï¿½ncia
scrapy crawl trf5 -a modo=... -s CONCURRENT_REQUESTS=1
```

### Problema: Dados extraï¿½dos incorretos

**Sintoma:**
```javascript
// MongoDB mostra campos vazios ou incorretos
db.processos.find({relator: "DESEMBARGADOR FEDERAL Joï¿½o"}) // tï¿½tulo nï¿½o removido
```

**Soluï¿½ï¿½o:**
```bash
# Reprocessar offline para testar extractors
scrapy crawl parse_raw -a limit=5 -s LOG_LEVEL=DEBUG

# Verificar normalizaï¿½ï¿½o
python3 -c "
from trf5_scraper.utils.normalize import normalize_relator
print(normalize_relator('DESEMBARGADOR FEDERAL Joï¿½o Silva'))
"
```

## =ï¿½ Mï¿½tricas de Sucesso

### Execuï¿½ï¿½o por NPU
-  6/6 NPUs processados com sucesso
-  Segunda execuï¿½ï¿½o de cada NPU gera "update" (nï¿½o "insert")
-  Todos campos obrigatï¿½rios preenchidos
-  Datas em formato ISO-8601

### Descoberta por CNPJ
-  Paginaï¿½ï¿½o detectada corretamente
-  Limites respeitados (max_pages, max_details_per_page)
-  Classificaï¿½ï¿½o correta: lista ï¿½ detalhes
-  Processos ï¿½nicos salvos (sem duplicatas)

### Reprocessamento Offline
-  Execuï¿½ï¿½o sem requisiï¿½ï¿½es de rede
-  Mesmos dados extraï¿½dos do HTML salvo
-  Logs "reprocessando" visï¿½veis

### Qualidade dos Dados
-  Relator sem prefixos ("Joï¿½o Silva" nï¿½o "Des. Joï¿½o Silva")
-  Envolvidos com papel e nome
-  Movimentaï¿½ï¿½es em ordem cronolï¿½gica
-  NPU usado como _id para idempotï¿½ncia

## <ï¿½ Checklist Final de Validaï¿½ï¿½o

Antes de considerar a execuï¿½ï¿½o completa:

```bash
# 1. Executar todos NPUs
./scripts/run_npu.sh

# 2. Testar descoberta CNPJ
./scripts/run_cnpj.sh

# 3. Verificar dados no MongoDB
./scripts/mongo_queries.sh

# 4. Testar reprocessamento
./scripts/reprocess_offline.sh

# 5. Executar QA completo
./scripts/claude/post_checks_trf5.sh

# 6. Verificar evidï¿½ncias
ls -la docs/evidencias/
```

### Critï¿½rios de Aceite Final

- [ ] 6 NPUs do BB salvos em `processos`
- [ ] Descoberta por CNPJ funcionando com limites
- [ ] HTML bruto salvo em `raw_pages`
- [ ] Idempotï¿½ncia comprovada (logs insertï¿½update)
- [ ] Reprocessamento offline operacional
- [ ] Polï¿½ticas de cortesia ativas
- [ ] Logs claros e informativos
- [ ] Consultas MongoDB retornam dados vï¿½lidos

---

**Nota:** Este runbook assume execuï¿½ï¿½o em ambiente Linux/Mac. Para Windows, adapte os comandos conforme necessï¿½rio.

**Contato:** Verificar logs detalhados em caso de problemas. Todos os comandos geram logs informativos para troubleshooting.